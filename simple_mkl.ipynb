{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dac85549-0f08-4e48-802b-983850a4f98c",
   "metadata": {},
   "source": [
    "### SimpleMKL Training\n",
    "\n",
    "- In scikit-learn we utilized a single kernel (radial basis function) with cross validated bandwidth (via grid search)\n",
    "\n",
    "- We now want to extend to the case of multiple kernels (linear combination of a basis set)\n",
    "\n",
    "- Inspiration in our original implementation from here \n",
    "\n",
    "     - https://github.com/qintian0321/SimpleMKL_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1f1b1ae-30b7-4af0-9474-607742e21729",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b577bc2-741c-4017-86a8-d592d53d66af",
   "metadata": {
    "tags": []
   },
   "source": [
    "# SimpleMKL optimization objective\n",
    "\n",
    "### Primal problem \n",
    "\n",
    "\n",
    "### Dual Problem\n",
    "\n",
    "$$\\max_\\alpha \\ \\frac{-1}{2} \\sum_{i,j} \\alpha_i \\alpha_j y_i y_j \\sum_m d_mK_m(x_i,x_j) +\\sum_i \\alpha_i$$\n",
    "\n",
    "s.t. $$\\sum_i \\alpha_i y_i=0$$ and $$C \\geq \\alpha_i \\geq 0$$\n",
    "\n",
    "\n",
    "\n",
    "- Coefficient vector $ \\alpha$ is importance of observed features on classification problem\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8bc243-3187-4e66-8dda-a1a2837d2dc9",
   "metadata": {},
   "source": [
    "### Abstract Kernel Class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ddddd267-59ac-420c-b85d-a72235090f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Kernel():\n",
    "    def __init__(self,kernel_type,order=None):\n",
    "        self.kernel_type=kernel_type\n",
    "        self.order=order\n",
    "    \n",
    "    def compute_kernel(self,X):\n",
    "        \n",
    "        if self.kernel_type=='linear':\n",
    "            return np.dot(X,X.T)\n",
    "        \n",
    "        if self.kernel_type=='gaussian':\n",
    "            return np.exp(X)\n",
    "        \n",
    "        if self.kernel_type=='polynomial':\n",
    "            return np.dot(X**self.order,X.T**self.order)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef18ca4-aa65-4a31-abe8-45f3e4ad44b5",
   "metadata": {},
   "source": [
    "### Functions for Multiple Kernel Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01c5bd8f-769f-44dd-b990-e9e644835f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_dual(X,y,kernel_list,d_m,constant,compute_gap=True):\n",
    "    \"\"\" Compute dual objective value\n",
    "    \"\"\"\n",
    "    kernel=compose_kernels(X,kernel_list,d_m)\n",
    "    single_kernel=svm.SVC(C=constant,kernel='precomputed')\n",
    "   \n",
    "    single_kernel.fit(kernel,y)\n",
    "    \n",
    "    alpha=np.empty(len(y))\n",
    "    alpha[single_kernel.support_]=np.abs(single_kernel.dual_coef_[0]) \n",
    "    alpha[alpha==None]=0\n",
    "    \n",
    "    \n",
    "    J=0.5*np.dot(np.dot(alpha,kernel*y),alpha.T)+np.sum(alpha)\n",
    "    \n",
    "    if compute_gap:\n",
    "        kernel_eval=[np.dot(np.dot(np.dot(alpha,alpha.T),np.dot(y,y.T)),k_i.compute_kernel(X)) for k_i in kernel_list]\n",
    "        duality_gap=J-np.sum(alpha)+0.5*np.max(kernel_eval)\n",
    "        \n",
    "        return J,duality_gap,alpha\n",
    "    \n",
    "    return J\n",
    "\n",
    "def compose_kernels(X,kernel_list,weights):\n",
    "    \"\"\" Compute positive linear combination of kernels \n",
    "    \"\"\"\n",
    "    return np.sum(np.array([weights[ct]*k_i.compute_kernel(X) for ct,k_i in enumerate(kernel_list)]),axis=0)\n",
    "    \n",
    "def compute_gradient(kernel,X,y,alpha):\n",
    "    \"\"\" Compute gradient of MKL objective closed form ; vector\n",
    "    \"\"\"\n",
    "    kernel_mat=kernel.compute_kernel(X)\n",
    "    gradient_obj=np.sum(np.dot(-0.5*np.dot(alpha.reshape(-1,1), np.dot(np.dot(alpha.reshape(1,-1),y.reshape(-1,1)) , y.reshape(1,-1))),kernel_mat))\n",
    "\n",
    "    return gradient_obj\n",
    "\n",
    "\n",
    "def descent_direction(m,d_m,mu,grad_dm,grad_mu,D):\n",
    "    \"\"\" Compute direction of gradient descent ; vector \n",
    "    \"\"\"\n",
    "    if d_m==0 and grad_dm-grad_mu>0:\n",
    "      \n",
    "        return 0\n",
    "    \n",
    "    elif d_m>0 and m!=mu:\n",
    "      \n",
    "        return -grad_dm+grad_mu\n",
    "    \n",
    "    elif m==mu:\n",
    "        a=np.arange(0,len(D))\n",
    "        b=np.array(mu)\n",
    "        grad_index=np.setdiff1d(a,b)\n",
    "        \n",
    "        \n",
    "        if len(grad_index)>0:\n",
    "            direction=np.sum(D[grad_index]-grad_mu)\n",
    "        else:\n",
    "            direction=-grad_mu\n",
    "        return direction\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def line_search(X,y,kernel_list,D,d_m,gamma_max,disc):\n",
    "    \"\"\" Selects step size to minimize obj value;  \n",
    "    \n",
    "        Update from heuristic to exact Armijo's rule \n",
    "    \"\"\"\n",
    "\n",
    "    if gamma_max==0:\n",
    "        return gamma_max\n",
    "    \n",
    "    # grid of step size begins bigger than 0\n",
    "    grid=np.arange(0+gamma_max/disc,gamma_max,gamma_max/disc)\n",
    "    \n",
    "    min_gamma,min_obj_val=None,10e8\n",
    "    for gamma_i in grid:\n",
    "        d_i=d_m+gamma_i*D\n",
    "        dual_obj_val=compute_dual(X,y,kernel_list,d_m,constant=100,compute_gap=False)\n",
    "        \n",
    "        if abs(dual_obj_val)<abs(min_obj_val):\n",
    "            min_obj_val=dual_obj_val\n",
    "            min_gamma=gamma_i\n",
    "        \n",
    "    \n",
    "    return min_gamma\n",
    "\n",
    "def primal_dual_opt(X,y,m,kernel_type,order,gap=10e-4,weight_threshold=0.01,maxiter=250, verbose=True):\n",
    "    \"\"\" X feature set, y are class outcomes\n",
    "        d_m is weight vector  on kernels, alpha is coefficient vector\n",
    "    \"\"\"\n",
    "    \n",
    "    duality_gap=1\n",
    "    C=0.01# penalization param\n",
    "    line_search_steps=25\n",
    "    n=len(y)\n",
    "    counter=0\n",
    "    gamma_max=0\n",
    "    \n",
    "    # optimziation init \n",
    "    d_m=np.ones(m)/m\n",
    "    D=np.ones(m)\n",
    "    mu=0\n",
    "    nu=0\n",
    "    \n",
    "    \n",
    "    kernel_list=[Kernel(kernel_type,i) for i in range(1,order+1)]\n",
    "\n",
    "    \n",
    "    # stopping criteria\n",
    "    while duality_gap>gap:\n",
    "        old_gap=duality_gap\n",
    "        if counter>maxiter:\n",
    "            return d_m\n",
    "        counter+=1\n",
    "        kernel=compose_kernels(X,kernel_list,d_m)\n",
    "     \n",
    "    \n",
    "        # compute svm objective\n",
    "        J_d,duality_gap,alpha=compute_dual(X,y,kernel_list,d_m,C) \n",
    "        if verbose:\n",
    "            print(\"Duality\",duality_gap)\n",
    "         \n",
    "        if abs(duality_gap-old_gap)<gap:\n",
    "            return d_m\n",
    "        \n",
    "        # gradient wrt each kernel\n",
    "        gradient_j=[compute_gradient(i,X,y,alpha) for i in kernel_list] \n",
    "        grad_mu=gradient_j[mu]\n",
    "   \n",
    "        # Need to investigate if normalization  occurs\n",
    "        D=np.array([descent_direction(i,d_m[i],mu,gradient_j[i],grad_mu,D) for i in range(0,len(gradient_j))])\n",
    "        norm_D=np.sqrt(D.dot(D))\n",
    "        mu=np.argmax(d_m)\n",
    "        D=D/norm_D\n",
    "        \n",
    "        J_hat=0\n",
    "        d_hat=d_m\n",
    "        D_hat=D\n",
    "        \n",
    "        \n",
    "        # descent direction update\n",
    "        ### Error -> Weight vector goes negative -> ratio goes negative -> step size goes negative\n",
    "        \n",
    "        \n",
    "        while J_hat<J_d: \n",
    "            \n",
    "            ratio_update=-d_hat[D_hat<0]/D_hat[D_hat<0]\n",
    "            if len(ratio_update)==0:\n",
    "                break\n",
    "            else:\n",
    "                ratio=ratio_update\n",
    "            \n",
    "            nu=np.argmin(ratio)\n",
    "\n",
    "            D=D_hat\n",
    "            d_m=d_hat\n",
    "\n",
    "            gamma_max=-d_m[nu]/D[nu]\n",
    "            \n",
    "           \n",
    "          \n",
    "            d_hat=d_m+gamma_max*D\n",
    "            d_hat[d_hat<weight_threshold]=0\n",
    "            \n",
    "            D_hat[mu]=D[mu]-D[nu]\n",
    "            D_hat[nu]=0\n",
    "          \n",
    "            J_hat=compute_dual(X,y,kernel_list,d_hat,C,compute_gap=False)\n",
    "            \n",
    "        # line search in descent direction  \n",
    "        gamma_step=line_search(X,y,kernel_list,D,d_m,gamma_max,disc=line_search_steps)\n",
    "        \n",
    "        d_m=(d_m+gamma_step*D)\n",
    "        \n",
    "        # normalize and drop threshold\n",
    "        d_m[d_m<weight_threshold]=0\n",
    "        d_m=d_m/np.sum(d_m)\n",
    "        \n",
    "        print(d_m)\n",
    "       \n",
    "        \n",
    "    return d_m\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18d90af1-4d44-4e24-9f53-1d3aa4fcade4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duality 2682.368110808049\n",
      "[0.24047638 0.18981959 0.18802484 0.19497584 0.18670335]\n",
      "Duality 2683.4348995650707\n",
      "[0.27896883 0.18013816 0.17663663 0.19019797 0.17405842]\n",
      "Duality 2991.602059711026\n",
      "[0.31561989 0.17091987 0.1657932  0.18564865 0.16201839]\n",
      "Duality 2988.0514268714815\n",
      "[0.3505586  0.16213227 0.15545639 0.18131188 0.15054087]\n",
      "Duality 2991.3059777475733\n",
      "[0.38390223 0.15374584 0.14559148 0.1771731  0.13958734]\n",
      "Duality 3002.00904172969\n",
      "[0.41575758 0.14573375 0.1361669  0.17321905 0.12912272]\n",
      "Duality 3623.3806713815616\n",
      "[0.44622211 0.13807147 0.12715379 0.16943764 0.11911499]\n",
      "Duality 3008.196578121404\n",
      "[0.47538496 0.13073657 0.1185258  0.1658178  0.10953487]\n",
      "Duality 3631.6525083760675\n",
      "[0.50332781 0.12370853 0.11025874 0.16234939 0.10035553]\n",
      "Duality 3329.139716098166\n",
      "[0.53012565 0.11696847 0.10233045 0.15902311 0.09155232]\n",
      "Duality 3332.454963174247\n",
      "[0.55584745 0.11049906 0.09472051 0.15583039 0.0831026 ]\n",
      "Duality 3642.858633202395\n",
      "[0.58055673 0.1042843  0.08741012 0.15276335 0.07498549]\n",
      "Duality 3338.687692527592\n",
      "[0.60431214 0.09830946 0.08038195 0.14981471 0.06718174]\n",
      "Duality 3956.9110813235657\n",
      "[0.62716786 0.09256091 0.07361995 0.14697774 0.05967354]\n",
      "Duality 3961.573088224023\n",
      "[0.64917406 0.08702602 0.0671093  0.14424622 0.05244441]\n",
      "Duality 3965.1225362028385\n",
      "[0.67037722 0.08169311 0.06083622 0.14161437 0.04547908]\n",
      "Duality 3658.7993912872234\n",
      "[0.69082053 0.07655131 0.05478795 0.13907685 0.03876336]\n",
      "Duality 3970.7409351752854\n",
      "[0.7105441  0.07159054 0.04895262 0.13662866 0.03228409]\n",
      "Duality 3974.0251537034287\n",
      "[0.72958529 0.0668014  0.04331918 0.13426517 0.02602897]\n",
      "Duality 3977.1722366549425\n",
      "[0.7479789  0.06217513 0.03787732 0.13198206 0.01998659]\n",
      "Duality 3979.8622430585356\n",
      "[0.76575743 0.05770356 0.03261744 0.1297753  0.01414627]\n",
      "Duality 3982.7139692590463\n",
      "[0.7896618  0.05383657 0.02776652 0.12873511 0.        ]\n",
      "Duality 3678.9462760683496\n",
      "[0.79819102 0.05321193 0.02457502 0.12402204 0.        ]\n",
      "Duality 3676.8646101571676\n",
      "[0.80652458 0.05260161 0.02145672 0.11941709 0.        ]\n",
      "Duality 3678.0535136849758\n",
      "[0.81466915 0.05200513 0.01840915 0.11491657 0.        ]\n",
      "Duality 3679.2276325157063\n",
      "[0.82263108 0.05142203 0.01542991 0.11051697 0.        ]\n",
      "Duality 3680.365323271238\n",
      "[0.83041645 0.05085186 0.01251675 0.10621494 0.        ]\n",
      "Duality 3994.8878054911806\n",
      "[0.84621179 0.05078516 0.         0.10300304 0.        ]\n",
      "Duality 3997.4510820597816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_25579/1741314469.py:158: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  gamma_max=-d_m[nu]/D[nu]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input X contains NaN.\nSVC does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 17>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m     d_m\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mround(primal_dual_opt(x,y,m,kernel_type,order\u001b[38;5;241m=\u001b[39mm,verbose\u001b[38;5;241m=\u001b[39mverbose),\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m d_m \n\u001b[0;32m---> 17\u001b[0m \u001b[43mtest_mkl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36mtest_mkl\u001b[0;34m(verbose)\u001b[0m\n\u001b[1;32m      8\u001b[0m x\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrand(n,\u001b[38;5;241m10\u001b[39m) \u001b[38;5;66;03m# # sample features\u001b[39;00m\n\u001b[1;32m      9\u001b[0m kernel_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpolynomial\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 12\u001b[0m d_m\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mround(\u001b[43mprimal_dual_opt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\u001b[43mkernel_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m,\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m d_m\n",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36mprimal_dual_opt\u001b[0;34m(X, y, m, kernel_type, order, gap, weight_threshold, maxiter, verbose)\u001b[0m\n\u001b[1;32m    165\u001b[0m     D_hat[mu]\u001b[38;5;241m=\u001b[39mD[mu]\u001b[38;5;241m-\u001b[39mD[nu]\n\u001b[1;32m    166\u001b[0m     D_hat[nu]\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 168\u001b[0m     J_hat\u001b[38;5;241m=\u001b[39m\u001b[43mcompute_dual\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43mkernel_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43md_hat\u001b[49m\u001b[43m,\u001b[49m\u001b[43mC\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcompute_gap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# line search in descent direction  \u001b[39;00m\n\u001b[1;32m    171\u001b[0m gamma_step\u001b[38;5;241m=\u001b[39mline_search(X,y,kernel_list,D,d_m,gamma_max,disc\u001b[38;5;241m=\u001b[39mline_search_steps)\n",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36mcompute_dual\u001b[0;34m(X, y, kernel_list, d_m, constant, compute_gap)\u001b[0m\n\u001b[1;32m      4\u001b[0m kernel\u001b[38;5;241m=\u001b[39mcompose_kernels(X,kernel_list,d_m)\n\u001b[1;32m      5\u001b[0m single_kernel\u001b[38;5;241m=\u001b[39msvm\u001b[38;5;241m.\u001b[39mSVC(C\u001b[38;5;241m=\u001b[39mconstant,kernel\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprecomputed\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m \u001b[43msingle_kernel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkernel\u001b[49m\u001b[43m,\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m alpha\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mempty(\u001b[38;5;28mlen\u001b[39m(y))\n\u001b[1;32m     10\u001b[0m alpha[single_kernel\u001b[38;5;241m.\u001b[39msupport_]\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mabs(single_kernel\u001b[38;5;241m.\u001b[39mdual_coef_[\u001b[38;5;241m0\u001b[39m]) \n",
      "File \u001b[0;32m~/miniconda3/envs/research/lib/python3.10/site-packages/sklearn/svm/_base.py:173\u001b[0m, in \u001b[0;36mBaseLibSVM.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    171\u001b[0m     check_consistent_length(X, y)\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 173\u001b[0m     X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat64\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    182\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_targets(y)\n\u001b[1;32m    184\u001b[0m sample_weight \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(\n\u001b[1;32m    185\u001b[0m     [] \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m sample_weight, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat64\n\u001b[1;32m    186\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/research/lib/python3.10/site-packages/sklearn/base.py:596\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    594\u001b[0m         y \u001b[38;5;241m=\u001b[39m check_array(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[1;32m    595\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 596\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_X_y\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    597\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[1;32m    599\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[0;32m~/miniconda3/envs/research/lib/python3.10/site-packages/sklearn/utils/validation.py:1074\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1069\u001b[0m         estimator_name \u001b[38;5;241m=\u001b[39m _check_estimator_name(estimator)\n\u001b[1;32m   1070\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1071\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires y to be passed, but the target y is None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1072\u001b[0m     )\n\u001b[0;32m-> 1074\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1075\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1076\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1077\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_large_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1078\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1079\u001b[0m \u001b[43m    \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1080\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1081\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1082\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_2d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1083\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1084\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_min_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_min_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1085\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_min_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_min_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1086\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1087\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1088\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1090\u001b[0m y \u001b[38;5;241m=\u001b[39m _check_y(y, multi_output\u001b[38;5;241m=\u001b[39mmulti_output, y_numeric\u001b[38;5;241m=\u001b[39my_numeric, estimator\u001b[38;5;241m=\u001b[39mestimator)\n\u001b[1;32m   1092\u001b[0m check_consistent_length(X, y)\n",
      "File \u001b[0;32m~/miniconda3/envs/research/lib/python3.10/site-packages/sklearn/utils/validation.py:899\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    893\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    894\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    895\u001b[0m             \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[1;32m    896\u001b[0m         )\n\u001b[1;32m    898\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[0;32m--> 899\u001b[0m         \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    900\u001b[0m \u001b[43m            \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    901\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    902\u001b[0m \u001b[43m            \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    903\u001b[0m \u001b[43m            \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    904\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    906\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_min_samples \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    907\u001b[0m     n_samples \u001b[38;5;241m=\u001b[39m _num_samples(array)\n",
      "File \u001b[0;32m~/miniconda3/envs/research/lib/python3.10/site-packages/sklearn/utils/validation.py:146\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    125\u001b[0m             \u001b[38;5;129;01mnot\u001b[39;00m allow_nan\n\u001b[1;32m    126\u001b[0m             \u001b[38;5;129;01mand\u001b[39;00m estimator_name\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[1;32m    131\u001b[0m             \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[1;32m    132\u001b[0m             msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    133\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    134\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    144\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    145\u001b[0m             )\n\u001b[0;32m--> 146\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n\u001b[1;32m    148\u001b[0m \u001b[38;5;66;03m# for object dtype data, we only check for NaNs (GH-13254)\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m X\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mdtype(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m allow_nan:\n",
      "\u001b[0;31mValueError\u001b[0m: Input X contains NaN.\nSVC does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
     ]
    }
   ],
   "source": [
    "def test_mkl(verbose=True):\n",
    "    \n",
    "    n=100 # data set size\n",
    "    \n",
    "    m= 5# num kernels\n",
    "    \n",
    "    y=np.sign(np.random.uniform(-1,1,size=n)) # sample class labels 1,-1\n",
    "    x=np.random.rand(n,10) # # sample features\n",
    "    kernel_type='polynomial'\n",
    "    \n",
    "  \n",
    "    d_m=np.round(primal_dual_opt(x,y,m,kernel_type,order=m,verbose=verbose),2)\n",
    "    \n",
    "    \n",
    "    return d_m \n",
    "\n",
    "test_mkl()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ef1f63-0265-4f35-a6fe-6d4ad2354594",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:research]",
   "language": "python",
   "name": "conda-env-research-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
