{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dac85549-0f08-4e48-802b-983850a4f98c",
   "metadata": {},
   "source": [
    "### SimpleMKL Training\n",
    "\n",
    "- In scikit-learn we utilized a single kernel (radial basis function) with cross validated bandwidth (via grid search)\n",
    "\n",
    "- We now want to extend to the case of multiple kernels (linear combination of a basis set)\n",
    "\n",
    "- Inspiration in our original implementation from here \n",
    "\n",
    "     - https://github.com/qintian0321/SimpleMKL_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a1f1b1ae-30b7-4af0-9474-607742e21729",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import svm\n",
    "from scipy.spatial import distance_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b577bc2-741c-4017-86a8-d592d53d66af",
   "metadata": {
    "tags": []
   },
   "source": [
    "# SimpleMKL optimization objective\n",
    "\n",
    "### Primal problem \n",
    "\n",
    "\n",
    "### Dual Problem\n",
    "\n",
    "$$\\max_\\alpha \\ \\frac{-1}{2} \\sum_{i,j} \\alpha_i \\alpha_j y_i y_j \\sum_m d_mK_m(x_i,x_j) +\\sum_i \\alpha_i$$\n",
    "\n",
    "s.t. $$\\sum_i \\alpha_i y_i=0$$ and $$C \\geq \\alpha_i \\geq 0$$\n",
    "\n",
    "\n",
    "\n",
    "- Coefficient vector $ \\alpha$ is importance of observed features on classification problem\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8bc243-3187-4e66-8dda-a1a2837d2dc9",
   "metadata": {},
   "source": [
    "### Abstract Kernel Class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ddddd267-59ac-420c-b85d-a72235090f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Kernel():\n",
    "    \"\"\" Abstract method to compute the kernel matrix under gaussian kernel or polynomial kernel\"\"\"\n",
    "    \n",
    "    def __init__(self,kernel_type,order=None,bandwidth=None):\n",
    "        self.kernel_type=kernel_type\n",
    "        self.order=order\n",
    "        self.bandwidth=bandwidth\n",
    "    \n",
    "    def compute_kernel(self,X):\n",
    "        \n",
    "        if self.kernel_type=='linear':\n",
    "            return np.dot(X,X.T)\n",
    "        \n",
    "        \n",
    "        if self.kernel_type=='gaussian':\n",
    "            # Compute the distance matrix which is then numerically transformed to gaussian kernel\n",
    "            scale=distance_matrix(X,X,p=2)\n",
    "            return np.exp(-0.5*self.bandwidth*(scale**2))\n",
    "        \n",
    "        if self.kernel_type=='polynomial':\n",
    "            return np.dot(X,X.T)**self.order\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c30843",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5ef18ca4-aa65-4a31-abe8-45f3e4ad44b5",
   "metadata": {},
   "source": [
    "### Functions for Multiple Kernel Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "01c5bd8f-769f-44dd-b990-e9e644835f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_dual(X,y,kernel_list,d_m,constant,compute_gap=True):\n",
    "    \"\"\" Compute dual objective value\n",
    "    \"\"\"\n",
    "    kernel=compose_kernels(X,kernel_list,d_m)\n",
    "    single_kernel=svm.SVC(C=constant,kernel='precomputed')\n",
    "    single_kernel.fit(kernel,y)\n",
    "    \n",
    "    alpha=np.empty(len(y))\n",
    "    alpha[single_kernel.support_]=np.abs(single_kernel.dual_coef_[0]) \n",
    "    alpha[alpha==None]=0\n",
    "    \n",
    "    \n",
    "    J=0.5*np.dot(np.dot(alpha,kernel*y),alpha.T)+np.sum(alpha)\n",
    "    \n",
    "    if compute_gap:\n",
    "        kernel_eval=[np.dot(np.dot(np.dot(alpha,alpha.T),np.dot(y,y.T)),k_i.compute_kernel(X)) for k_i in kernel_list]\n",
    "        duality_gap=J-np.sum(alpha)+0.5*np.max(kernel_eval)\n",
    "        \n",
    "        return J,duality_gap,alpha\n",
    "    \n",
    "    return J\n",
    "\n",
    "def compose_kernels(X,kernel_list,weights):\n",
    "    \"\"\" Compute positive linear combination of kernels \n",
    "    \"\"\"\n",
    "    return np.sum(np.array([weights[ct]*k_i.compute_kernel(X) for ct,k_i in enumerate(kernel_list)]),axis=0)\n",
    "    \n",
    "def compute_gradient(kernel,X,y_outer,alpha):\n",
    "    \"\"\" Compute gradient of MKL objective closed form ; vector\n",
    "    \"\"\"\n",
    "    kernel_mat=kernel.compute_kernel(X)\n",
    "    \n",
    "    # utilizes outer product for improved efficiency\n",
    "    gradient_obj=-0.5*np.dot(np.dot(alpha,np.multiply(y_outer,kernel_mat)),alpha.T)\n",
    "    \n",
    "    return gradient_obj\n",
    "\n",
    "\n",
    "def descent_direction(d_m,mu,gradient_j,grad_mu):\n",
    "    \"\"\" Compute direction of gradient descent ; vector \n",
    "    \"\"\"\n",
    "    n=len(d_m)\n",
    "    D=np.zeros(n)\n",
    "    ongoing_sum=0\n",
    "    for index in range(0,n):\n",
    "        if d_m[index]==0 and gradient_j[index]-grad_mu>0:\n",
    "            D[index]=0\n",
    "       \n",
    "    \n",
    "        elif d_m[index]>0 and index!=mu:\n",
    "            \n",
    "            grad_m=-gradient_j[index]+grad_mu\n",
    "            D[index]=grad_m\n",
    "            ongoing_sum+=grad_m\n",
    "            \n",
    "        \n",
    "        else:\n",
    "            D[index]=0\n",
    "    \n",
    "    # might not be negative     \n",
    "    D[mu]=-ongoing_sum\n",
    "\n",
    "    return D\n",
    "    \n",
    "    \n",
    "def line_search(X,y,kernel_list,D,d_m,gamma_max,disc):\n",
    "    \"\"\" Selects step size to minimize obj value;  \n",
    "    \n",
    "        Update from heuristic to exact Armijo's rule \n",
    "    \"\"\"\n",
    "\n",
    "    if gamma_max==0:\n",
    "        return gamma_max\n",
    "    \n",
    "    # grid of step size begins bigger than 0\n",
    "    grid=np.arange(0+gamma_max/disc,gamma_max,gamma_max/disc)\n",
    "    \n",
    "    min_gamma,min_obj_val=None,10e8\n",
    "    for gamma_i in grid:\n",
    "        d_i=d_m+gamma_i*D\n",
    "        dual_obj_val=compute_dual(X,y,kernel_list,d_m,constant=100,compute_gap=False)\n",
    "        \n",
    "        if abs(dual_obj_val)<abs(min_obj_val):\n",
    "            min_obj_val=dual_obj_val\n",
    "            min_gamma=gamma_i\n",
    "        \n",
    "    \n",
    "    return min_gamma\n",
    "\n",
    "def primal_dual_opt(X,y,m,kernel_type,order,gap=10e-4,inner_tol=10e-1,weight_threshold=0.01,maxiter=100, verbose=True):\n",
    "    \"\"\" X feature set, y are class outcomes\n",
    "        d_m is weight vector  on kernels, alpha is coefficient vector\n",
    "    \"\"\"\n",
    "    \n",
    "    duality_gap=1\n",
    "    C=0.01# penalization param\n",
    "    line_search_steps=25\n",
    "    n=len(y)\n",
    "    counter=0\n",
    "    gamma_max=0\n",
    "    y_outer=np.outer(y,y)\n",
    "    \n",
    "    # optimziation init \n",
    "    d_m=np.ones(m)/m\n",
    "    D=np.ones(m)\n",
    "    mu=0\n",
    "    nu=0\n",
    "    \n",
    "    if kernel_type=='linear':\n",
    "        kernel_list=[Kernel(kernel_type=kernel_type)for i in range(1,order+1) ]\n",
    "    elif kernel_type=='polynomial':\n",
    "        \n",
    "        kernel_list=[Kernel(kernel_type,i) for i in range(1,order+1)]\n",
    "    elif kernel_type=='gaussian':\n",
    "        kernel_list=[Kernel(kernel_type,bandwidth=i) for i in np.linspace(0.1,1,m)] # gamma hyperparam \n",
    "\n",
    "    else:\n",
    "        print(\"Not Valid Kernel Type\")\n",
    "        return\n",
    "    \n",
    "    # stopping criteria\n",
    "    while duality_gap>gap and gap>=0:\n",
    "        old_gap=duality_gap\n",
    "        if counter>maxiter:\n",
    "            return d_m\n",
    "        counter+=1\n",
    "\n",
    "        # compute svm objective\n",
    "        J_d,duality_gap,alpha=compute_dual(X,y,kernel_list,d_m,C) \n",
    "        if verbose:\n",
    "            print(\"Duality\",duality_gap)\n",
    "         \n",
    "        if abs(duality_gap-old_gap)<gap:\n",
    "            return d_m\n",
    "        \n",
    "        print(\"Current Guess\", d_m)\n",
    "        # gradient wrt each kernel\n",
    "        gradient_j=[compute_gradient(i,X,y_outer,alpha) for i in kernel_list] \n",
    "        print(\"Gradient is \",gradient_j)\n",
    "        grad_mu=gradient_j[mu]\n",
    "   \n",
    "        # computes descent direction ; normalized for equality constraints \n",
    "        D=descent_direction(d_m,mu,gradient_j,grad_mu)\n",
    "        \n",
    "        norm_D=np.sqrt(D.dot(D))\n",
    "        D=D/norm_D\n",
    "        print(\"Descent Direction is \",D)\n",
    "        mu=np.argmax(d_m)\n",
    "\n",
    "        print(d_m)\n",
    "\n",
    "        \n",
    "        J_hat=0\n",
    "        d_hat=d_m\n",
    "        D_hat=D\n",
    "        \n",
    "        \n",
    "        # descent direction update\n",
    "        inner_iter=0\n",
    "        while J_hat+inner_tol<J_d or inner_iter<maxiter:\n",
    "            inner_iter+=1 \n",
    "            \n",
    "            # indices where descent direction is negative \n",
    "            nonzero_D=np.where(D_hat<0)[0]\n",
    "            \n",
    "            if len(nonzero_D)==0:\n",
    "                # if none we have reached local minimia \n",
    "                gamma_max=0  \n",
    "                \n",
    "            else:\n",
    "                # else step gamma size\n",
    "                gamma_max=np.min(-d_hat[nonzero_D]/D_hat[nonzero_D])\n",
    "      \n",
    "            D=D_hat\n",
    "            d_m=d_hat\n",
    "\n",
    "            d_hat=d_m+gamma_max*D\n",
    "            d_hat[d_hat<weight_threshold]=0\n",
    "                \n",
    "            # need to fix descent step here \n",
    "            #######################\n",
    "            D_hat[mu]=D[mu]-D[nu]\n",
    "            D_hat[nu]=0\n",
    "            \n",
    "            J_hat=compute_dual(X,y,kernel_list,d_hat,C,compute_gap=False)\n",
    "            print(\"J_hat\",J_hat,\"J_d\",J_d)\n",
    "            #####################\n",
    "            \n",
    "        # line search in descent direction  \n",
    "        gamma_step=line_search(X,y,kernel_list,D,d_m,gamma_max,disc=line_search_steps)\n",
    "        \n",
    "        d_m=(d_m+gamma_step*D)\n",
    "        \n",
    "        # normalize and drop threshold\n",
    "        d_m[d_m<weight_threshold]=0\n",
    "        d_m=d_m/np.sum(d_m)\n",
    "        \n",
    "        print(\"NEW\",d_m)\n",
    "       \n",
    "        \n",
    "    return d_m\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "18d90af1-4d44-4e24-9f53-1d3aa4fcade4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duality 1189679.2650627315\n",
      "Current Guess [0.2 0.2 0.2 0.2 0.2]\n",
      "Gradient is  [-18.738459840682864, -80.49719973743147, -359.5692954701435, -1676.6685636008094, -8165.2137079557215]\n",
      "Descent Direction is  [-0.77509096  0.00468979  0.02588175  0.12589863  0.61862079]\n",
      "[0.2 0.2 0.2 0.2 0.2]\n",
      "J_hat 6533.972298681651 J_d 2221.8232971641037\n",
      "J_hat 6533.972298681651 J_d 2221.8232971641037\n",
      "J_hat 6533.972298681651 J_d 2221.8232971641037\n",
      "J_hat 6533.972298681651 J_d 2221.8232971641037\n",
      "J_hat 6533.972298681651 J_d 2221.8232971641037\n",
      "J_hat 6533.972298681651 J_d 2221.8232971641037\n",
      "J_hat 6533.972298681651 J_d 2221.8232971641037\n",
      "J_hat 6533.972298681651 J_d 2221.8232971641037\n",
      "J_hat 6533.972298681651 J_d 2221.8232971641037\n",
      "J_hat 6533.972298681651 J_d 2221.8232971641037\n",
      "J_hat 6533.972298681651 J_d 2221.8232971641037\n",
      "J_hat 6533.972298681651 J_d 2221.8232971641037\n",
      "J_hat 6533.972298681651 J_d 2221.8232971641037\n",
      "J_hat 6533.972298681651 J_d 2221.8232971641037\n",
      "J_hat 6533.972298681651 J_d 2221.8232971641037\n",
      "J_hat 6533.972298681651 J_d 2221.8232971641037\n",
      "J_hat 6533.972298681651 J_d 2221.8232971641037\n",
      "J_hat 6533.972298681651 J_d 2221.8232971641037\n",
      "J_hat 6533.972298681651 J_d 2221.8232971641037\n",
      "J_hat 6533.972298681651 J_d 2221.8232971641037\n",
      "J_hat 6533.972298681651 J_d 2221.8232971641037\n",
      "J_hat 6533.972298681651 J_d 2221.8232971641037\n",
      "J_hat 6533.972298681651 J_d 2221.8232971641037\n",
      "J_hat 6533.972298681651 J_d 2221.8232971641037\n",
      "J_hat 6533.972298681651 J_d 2221.8232971641037\n",
      "J_hat 6533.972298681651 J_d 2221.8232971641037\n",
      "J_hat 6533.972298681651 J_d 2221.8232971641037\n",
      "J_hat 6533.972298681651 J_d 2221.8232971641037\n",
      "J_hat 6533.972298681651 J_d 2221.8232971641037\n",
      "J_hat 6533.972298681651 J_d 2221.8232971641037\n",
      "J_hat 6533.972298681651 J_d 2221.8232971641037\n",
      "J_hat 6533.972298681651 J_d 2221.8232971641037\n",
      "J_hat 6533.972298681651 J_d 2221.8232971641037\n",
      "J_hat 6533.972298681651 J_d 2221.8232971641037\n",
      "J_hat 6533.972298681651 J_d 2221.8232971641037\n",
      "J_hat 6533.972298681651 J_d 2221.8232971641037\n",
      "J_hat 6533.972298681651 J_d 2221.8232971641037\n",
      "J_hat 6533.972298681651 J_d 2221.8232971641037\n",
      "J_hat 6533.972298681651 J_d 2221.8232971641037\n",
      "J_hat 6533.972298681651 J_d 2221.8232971641037\n",
      "J_hat 6533.972298681651 J_d 2221.8232971641037\n",
      "J_hat 6533.972298681651 J_d 2221.8232971641037\n",
      "J_hat 6533.972298681651 J_d 2221.8232971641037\n",
      "J_hat 6533.972298681651 J_d 2221.8232971641037\n",
      "J_hat 6533.972298681651 J_d 2221.8232971641037\n",
      "J_hat 6533.972298681651 J_d 2221.8232971641037\n",
      "J_hat 6533.972298681651 J_d 2221.8232971641037\n",
      "J_hat 6533.972298681651 J_d 2221.8232971641037\n",
      "J_hat 6533.972298681651 J_d 2221.8232971641037\n",
      "J_hat 6533.972298681651 J_d 2221.8232971641037\n",
      "J_hat 6533.972298681651 J_d 2221.8232971641037\n",
      "J_hat 6533.972298681651 J_d 2221.8232971641037\n",
      "J_hat 6533.972298681651 J_d 2221.8232971641037\n",
      "J_hat 6533.972298681651 J_d 2221.8232971641037\n",
      "J_hat 6533.972298681651 J_d 2221.8232971641037\n",
      "J_hat 6533.972298681651 J_d 2221.8232971641037\n",
      "J_hat 6533.972298681651 J_d 2221.8232971641037\n",
      "J_hat 6533.972298681651 J_d 2221.8232971641037\n",
      "J_hat 6533.972298681651 J_d 2221.8232971641037\n",
      "J_hat 6533.972298681651 J_d 2221.8232971641037\n",
      "J_hat 6533.972298681651 J_d 2221.8232971641037\n",
      "J_hat 6533.972298681651 J_d 2221.8232971641037\n",
      "J_hat 6533.972298681651 J_d 2221.8232971641037\n",
      "J_hat 6533.972298681651 J_d 2221.8232971641037\n",
      "J_hat 6533.972298681651 J_d 2221.8232971641037\n",
      "J_hat 6533.972298681651 J_d 2221.8232971641037\n",
      "J_hat 6533.972298681651 J_d 2221.8232971641037\n",
      "J_hat 6533.972298681651 J_d 2221.8232971641037\n",
      "J_hat 6533.972298681651 J_d 2221.8232971641037\n",
      "J_hat 6533.972298681651 J_d 2221.8232971641037\n",
      "J_hat 6533.972298681651 J_d 2221.8232971641037\n",
      "J_hat 6533.972298681651 J_d 2221.8232971641037\n",
      "J_hat 6533.972298681651 J_d 2221.8232971641037\n",
      "J_hat 6533.972298681651 J_d 2221.8232971641037\n",
      "J_hat 6533.972298681651 J_d 2221.8232971641037\n",
      "J_hat 6533.972298681651 J_d 2221.8232971641037\n",
      "J_hat 6533.972298681651 J_d 2221.8232971641037\n",
      "J_hat 6533.972298681651 J_d 2221.8232971641037\n",
      "J_hat 6533.972298681651 J_d 2221.8232971641037\n",
      "J_hat 6533.972298681651 J_d 2221.8232971641037\n",
      "J_hat 6533.972298681651 J_d 2221.8232971641037\n",
      "J_hat 6533.972298681651 J_d 2221.8232971641037\n",
      "J_hat 6533.972298681651 J_d 2221.8232971641037\n",
      "J_hat 6533.972298681651 J_d 2221.8232971641037\n",
      "J_hat 6533.972298681651 J_d 2221.8232971641037\n",
      "J_hat 6533.972298681651 J_d 2221.8232971641037\n",
      "J_hat 6533.972298681651 J_d 2221.8232971641037\n",
      "J_hat 6533.972298681651 J_d 2221.8232971641037\n",
      "J_hat 6533.972298681651 J_d 2221.8232971641037\n",
      "J_hat 6533.972298681651 J_d 2221.8232971641037\n",
      "J_hat 6533.972298681651 J_d 2221.8232971641037\n",
      "J_hat 6533.972298681651 J_d 2221.8232971641037\n",
      "J_hat 6533.972298681651 J_d 2221.8232971641037\n",
      "J_hat 6533.972298681651 J_d 2221.8232971641037\n",
      "J_hat 6533.972298681651 J_d 2221.8232971641037\n",
      "J_hat 6533.972298681651 J_d 2221.8232971641037\n",
      "J_hat 6533.972298681651 J_d 2221.8232971641037\n",
      "J_hat 6533.972298681651 J_d 2221.8232971641037\n",
      "J_hat 6533.972298681651 J_d 2221.8232971641037\n",
      "J_hat 6533.972298681651 J_d 2221.8232971641037\n",
      "NEW [0.         0.20121013 0.20667838 0.23248616 0.35962534]\n",
      "Duality 1983502.5502456059\n",
      "Current Guess [0.         0.20121013 0.20667838 0.23248616 0.35962534]\n",
      "Gradient is  [-47.55658853199674, -187.52543248612406, -768.2613102688242, -3276.383473665096, -14564.483270697314]\n",
      "Descent Direction is  [-0.7807739   0.00587346  0.03024264  0.13548994  0.60916786]\n",
      "[0.         0.20121013 0.20667838 0.23248616 0.35962534]\n",
      "J_hat 6533.972298681651 J_d 6533.972298681651\n",
      "J_hat 6533.972298681651 J_d 6533.972298681651\n",
      "J_hat 6533.972298681651 J_d 6533.972298681651\n",
      "J_hat 6533.972298681651 J_d 6533.972298681651\n",
      "J_hat 6533.972298681651 J_d 6533.972298681651\n",
      "J_hat 6533.972298681651 J_d 6533.972298681651\n",
      "J_hat 6533.972298681651 J_d 6533.972298681651\n",
      "J_hat 6533.972298681651 J_d 6533.972298681651\n",
      "J_hat 6533.972298681651 J_d 6533.972298681651\n",
      "J_hat 6533.972298681651 J_d 6533.972298681651\n",
      "J_hat 6533.972298681651 J_d 6533.972298681651\n",
      "J_hat 6533.972298681651 J_d 6533.972298681651\n",
      "J_hat 6533.972298681651 J_d 6533.972298681651\n",
      "J_hat 6533.972298681651 J_d 6533.972298681651\n",
      "J_hat 6533.972298681651 J_d 6533.972298681651\n",
      "J_hat 6533.972298681651 J_d 6533.972298681651\n",
      "J_hat 6533.972298681651 J_d 6533.972298681651\n",
      "J_hat 6533.972298681651 J_d 6533.972298681651\n",
      "J_hat 6533.972298681651 J_d 6533.972298681651\n",
      "J_hat 6533.972298681651 J_d 6533.972298681651\n",
      "J_hat 6533.972298681651 J_d 6533.972298681651\n",
      "J_hat 6533.972298681651 J_d 6533.972298681651\n",
      "J_hat 6533.972298681651 J_d 6533.972298681651\n",
      "J_hat 6533.972298681651 J_d 6533.972298681651\n",
      "J_hat 6533.972298681651 J_d 6533.972298681651\n",
      "J_hat 6533.972298681651 J_d 6533.972298681651\n",
      "J_hat 6533.972298681651 J_d 6533.972298681651\n",
      "J_hat 6533.972298681651 J_d 6533.972298681651\n",
      "J_hat 6533.972298681651 J_d 6533.972298681651\n",
      "J_hat 6533.972298681651 J_d 6533.972298681651\n",
      "J_hat 6533.972298681651 J_d 6533.972298681651\n",
      "J_hat 6533.972298681651 J_d 6533.972298681651\n",
      "J_hat 6533.972298681651 J_d 6533.972298681651\n",
      "J_hat 6533.972298681651 J_d 6533.972298681651\n",
      "J_hat 6533.972298681651 J_d 6533.972298681651\n",
      "J_hat 6533.972298681651 J_d 6533.972298681651\n",
      "J_hat 6533.972298681651 J_d 6533.972298681651\n",
      "J_hat 6533.972298681651 J_d 6533.972298681651\n",
      "J_hat 6533.972298681651 J_d 6533.972298681651\n",
      "J_hat 6533.972298681651 J_d 6533.972298681651\n",
      "J_hat 6533.972298681651 J_d 6533.972298681651\n",
      "J_hat 6533.972298681651 J_d 6533.972298681651\n",
      "J_hat 6533.972298681651 J_d 6533.972298681651\n",
      "J_hat 6533.972298681651 J_d 6533.972298681651\n",
      "J_hat 6533.972298681651 J_d 6533.972298681651\n",
      "J_hat 6533.972298681651 J_d 6533.972298681651\n",
      "J_hat 6533.972298681651 J_d 6533.972298681651\n",
      "J_hat 6533.972298681651 J_d 6533.972298681651\n",
      "J_hat 6533.972298681651 J_d 6533.972298681651\n",
      "J_hat 6533.972298681651 J_d 6533.972298681651\n",
      "J_hat 6533.972298681651 J_d 6533.972298681651\n",
      "J_hat 6533.972298681651 J_d 6533.972298681651\n",
      "J_hat 6533.972298681651 J_d 6533.972298681651\n",
      "J_hat 6533.972298681651 J_d 6533.972298681651\n",
      "J_hat 6533.972298681651 J_d 6533.972298681651\n",
      "J_hat 6533.972298681651 J_d 6533.972298681651\n",
      "J_hat 6533.972298681651 J_d 6533.972298681651\n",
      "J_hat 6533.972298681651 J_d 6533.972298681651\n",
      "J_hat 6533.972298681651 J_d 6533.972298681651\n",
      "J_hat 6533.972298681651 J_d 6533.972298681651\n",
      "J_hat 6533.972298681651 J_d 6533.972298681651\n",
      "J_hat 6533.972298681651 J_d 6533.972298681651\n",
      "J_hat 6533.972298681651 J_d 6533.972298681651\n",
      "J_hat 6533.972298681651 J_d 6533.972298681651\n",
      "J_hat 6533.972298681651 J_d 6533.972298681651\n",
      "J_hat 6533.972298681651 J_d 6533.972298681651\n",
      "J_hat 6533.972298681651 J_d 6533.972298681651\n",
      "J_hat 6533.972298681651 J_d 6533.972298681651\n",
      "J_hat 6533.972298681651 J_d 6533.972298681651\n",
      "J_hat 6533.972298681651 J_d 6533.972298681651\n",
      "J_hat 6533.972298681651 J_d 6533.972298681651\n",
      "J_hat 6533.972298681651 J_d 6533.972298681651\n",
      "J_hat 6533.972298681651 J_d 6533.972298681651\n",
      "J_hat 6533.972298681651 J_d 6533.972298681651\n",
      "J_hat 6533.972298681651 J_d 6533.972298681651\n",
      "J_hat 6533.972298681651 J_d 6533.972298681651\n",
      "J_hat 6533.972298681651 J_d 6533.972298681651\n",
      "J_hat 6533.972298681651 J_d 6533.972298681651\n",
      "J_hat 6533.972298681651 J_d 6533.972298681651\n",
      "J_hat 6533.972298681651 J_d 6533.972298681651\n",
      "J_hat 6533.972298681651 J_d 6533.972298681651\n",
      "J_hat 6533.972298681651 J_d 6533.972298681651\n",
      "J_hat 6533.972298681651 J_d 6533.972298681651\n",
      "J_hat 6533.972298681651 J_d 6533.972298681651\n",
      "J_hat 6533.972298681651 J_d 6533.972298681651\n",
      "J_hat 6533.972298681651 J_d 6533.972298681651\n",
      "J_hat 6533.972298681651 J_d 6533.972298681651\n",
      "J_hat 6533.972298681651 J_d 6533.972298681651\n",
      "J_hat 6533.972298681651 J_d 6533.972298681651\n",
      "J_hat 6533.972298681651 J_d 6533.972298681651\n",
      "J_hat 6533.972298681651 J_d 6533.972298681651\n",
      "J_hat 6533.972298681651 J_d 6533.972298681651\n",
      "J_hat 6533.972298681651 J_d 6533.972298681651\n",
      "J_hat 6533.972298681651 J_d 6533.972298681651\n",
      "J_hat 6533.972298681651 J_d 6533.972298681651\n",
      "J_hat 6533.972298681651 J_d 6533.972298681651\n",
      "J_hat 6533.972298681651 J_d 6533.972298681651\n",
      "J_hat 6533.972298681651 J_d 6533.972298681651\n",
      "J_hat 6533.972298681651 J_d 6533.972298681651\n",
      "J_hat 6533.972298681651 J_d 6533.972298681651\n",
      "NEW [0.         0.20121013 0.20667838 0.23248616 0.35962534]\n",
      "Duality 1983502.5502456059\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.  , 0.2 , 0.21, 0.23, 0.36])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def test_mkl(verbose=True):\n",
    "    \n",
    "    n=100 # data set size\n",
    "    \n",
    "    m=5 #6 num kernels\n",
    "    \n",
    "    y=np.sign(np.random.uniform(-1,1,size=n)) # sample class labels 1,-1\n",
    "    x=np.random.rand(n,10) # # sample features\n",
    "    kernel_type='polynomial'\n",
    "    \n",
    "  \n",
    "    d_m=np.round(primal_dual_opt(x,y,m,kernel_type,order=m,verbose=verbose),2)\n",
    "    \n",
    "    \n",
    "    return d_m \n",
    "\n",
    "test_mkl()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "8a18061b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=np.array([])\n",
    "len(np.where(a<0)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ef1f63-0265-4f35-a6fe-6d4ad2354594",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3],\n",
       "       [2, 4, 6],\n",
       "       [3, 6, 9]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=np.array([1,2,3])\n",
    "b=np.array([4,5,6])\n",
    "\n",
    "np.outer(a,a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b632aa83",
   "metadata": {},
   "source": [
    "### Issues at the moment\n",
    "- Gradient isn't always negative; which means there is an issue in computation as this corresponds to a decrease\n",
    "\n",
    "SVM includes NAN values\n",
    "- Gaussian Kernels give equidistant results "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50067003",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('research')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "8fefc22b81b58a95d090389e6427ac7414ba2434182c3be0d4e0279b8ec28ded"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
