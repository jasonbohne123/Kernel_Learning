{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dac85549-0f08-4e48-802b-983850a4f98c",
   "metadata": {},
   "source": [
    "### SimpleMKL Training\n",
    "\n",
    "- In scikit-learn we utilized a single kernel (radial basis function) with cross validated bandwidth (via grid search)\n",
    "\n",
    "- We now want to extend to the case of multiple kernels (linear combination of a basis set)\n",
    "\n",
    "- Inspiration in our original implementation from here \n",
    "\n",
    "     - https://github.com/qintian0321/SimpleMKL_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1f1b1ae-30b7-4af0-9474-607742e21729",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import svm\n",
    "from scipy.spatial import distance_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b577bc2-741c-4017-86a8-d592d53d66af",
   "metadata": {
    "tags": []
   },
   "source": [
    "# SimpleMKL optimization objective\n",
    "\n",
    "### Primal problem \n",
    "\n",
    "\n",
    "### Dual Problem\n",
    "\n",
    "$$\\max_\\alpha \\ \\frac{-1}{2} \\sum_{i,j} \\alpha_i \\alpha_j y_i y_j \\sum_m d_mK_m(x_i,x_j) +\\sum_i \\alpha_i$$\n",
    "\n",
    "s.t. $$\\sum_i \\alpha_i y_i=0$$ and $$C \\geq \\alpha_i \\geq 0$$\n",
    "\n",
    "\n",
    "\n",
    "- Coefficient vector $ \\alpha$ is importance of observed features on classification problem\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8bc243-3187-4e66-8dda-a1a2837d2dc9",
   "metadata": {},
   "source": [
    "### Abstract Kernel Class \n",
    "\n",
    "- Define an abstract kernel class wich allows for the computation of polynomial or gaussian kernels of varying order /bandwidth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ddddd267-59ac-420c-b85d-a72235090f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Kernel():\n",
    "    \"\"\" Abstract method to compute the kernel matrix under gaussian kernel or polynomial kernel\"\"\"\n",
    "    \n",
    "    def __init__(self,kernel_type,order=None,bandwidth=None):\n",
    "        self.kernel_type=kernel_type\n",
    "        self.order=order\n",
    "        self.bandwidth=bandwidth\n",
    "    \n",
    "    def compute_kernel(self,X):\n",
    "        \n",
    "        if self.kernel_type=='linear':\n",
    "            return np.dot(X,X.T)\n",
    "        \n",
    "        \n",
    "        if self.kernel_type=='gaussian':\n",
    "            # Compute the distance matrix which is then numerically transformed to gaussian kernel\n",
    "            scale=distance_matrix(X,X,p=2)\n",
    "            return np.exp(-0.5*self.bandwidth*(scale**2))\n",
    "        \n",
    "        if self.kernel_type=='polynomial':\n",
    "            return np.dot(X,X.T)**self.order\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa7a5cd",
   "metadata": {},
   "source": [
    "### Compose a linear combination of Kernel Matrices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04078d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compose_kernels(X,kernel_list,weights):\n",
    "    \"\"\" Compute positive linear combination of kernels \n",
    "    \"\"\"\n",
    "    return np.sum(np.array([weights[ct]*k_i.compute_kernel(X) for ct,k_i in enumerate(kernel_list)]),axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef18ca4-aa65-4a31-abe8-45f3e4ad44b5",
   "metadata": {},
   "source": [
    "### Functions for Multiple Kernel Optimization\n",
    "\n",
    "- Objective is Optimized via Gradient Descent\n",
    "- Main functionality required\n",
    "\n",
    "### Compute Dual objective and Duality Gap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9d6c292",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_dual(X,y,kernel_list,d_m,constant,compute_gap=True):\n",
    "    \"\"\" Compute dual objective value for single SVM\n",
    "    \"\"\"\n",
    "    kernel=compose_kernels(X,kernel_list,d_m)\n",
    "    single_kernel=svm.SVC(C=constant,kernel='precomputed')\n",
    "    single_kernel.fit(kernel,y)\n",
    "    \n",
    "    alpha=np.empty(len(y))\n",
    "    alpha[single_kernel.support_]=np.abs(single_kernel.dual_coef_[0]) \n",
    "    alpha[alpha==None]=0\n",
    "\n",
    "    y_outer=np.outer(y,y)\n",
    "    \n",
    "    J=-0.5*np.dot(np.dot(alpha,np.multiply(y_outer,kernel)),alpha.T)+np.sum(alpha)\n",
    "    \n",
    "    if compute_gap:\n",
    "        kernel_eval=[np.dot(np.dot(alpha,np.multiply(y_outer,k_i.compute_kernel(X))),alpha.T) for k_i in kernel_list]\n",
    "        duality_gap=J-np.sum(alpha)+0.5*np.max(kernel_eval)\n",
    "        \n",
    "        return J,duality_gap,alpha\n",
    "    \n",
    "    return J"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e1606f",
   "metadata": {},
   "source": [
    "### Compute the Gradient with respect to kernel weight vector $d_M$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "437ffeef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(kernel,X,y_outer,alpha):\n",
    "    \"\"\" Compute gradient of MKL objective closed form ; vector\n",
    "    \"\"\"\n",
    "    kernel_mat=kernel.compute_kernel(X)\n",
    "    gradient_obj=-0.5*np.dot(np.dot(alpha,np.multiply(y_outer,kernel_mat)),alpha.T)\n",
    "    \n",
    "    return gradient_obj"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dbaf237",
   "metadata": {},
   "source": [
    "### Compute Direction of Descent respecting equality conditions\n",
    "- Most optimal direction might violate normality and nonnegative conditons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b76c30a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def descent_direction(d_m,mu,gradient_j,grad_mu):\n",
    "    \"\"\" Compute direction of gradient descent ; vector \n",
    "    \"\"\"\n",
    "    n=len(d_m)\n",
    "    D=np.zeros(n)\n",
    "    ongoing_sum=0\n",
    "    for index in range(0,n):\n",
    "        if d_m[index]==0 and gradient_j[index]-grad_mu>0:\n",
    "            D[index]=0\n",
    "    \n",
    "        elif d_m[index]>0 and index!=mu:\n",
    "            \n",
    "            grad_m=-gradient_j[index]+grad_mu\n",
    "            D[index]=grad_m\n",
    "            ongoing_sum+=grad_m  \n",
    "        \n",
    "        else:\n",
    "            D[index]=0\n",
    "         \n",
    "    D[mu]=-ongoing_sum\n",
    "\n",
    "    return D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12a234c",
   "metadata": {},
   "source": [
    "### Line Search for Optimal Step Size $gamma_{max}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "938bfb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def line_search(X,y,kernel_list,D,d_m,gamma_max,disc):\n",
    "    \"\"\" Selects step size to minimize obj value;  \n",
    "    \n",
    "        Update from heuristic to exact Armijo's rule \n",
    "    \"\"\"\n",
    "\n",
    "    if gamma_max==0:\n",
    "        return gamma_max\n",
    "    \n",
    "    # grid of step size begins bigger than 0\n",
    "    grid=np.arange(0+gamma_max/disc,gamma_max,gamma_max/disc)\n",
    "    \n",
    "    min_gamma,min_obj_val=None,10e8\n",
    "    for gamma_i in grid:\n",
    "        d_i=d_m+gamma_i*D\n",
    "        dual_obj_val=compute_dual(X,y,kernel_list,d_i,constant=100,compute_gap=False)\n",
    "        \n",
    "        if abs(dual_obj_val)<abs(min_obj_val):\n",
    "            min_obj_val=dual_obj_val\n",
    "            min_gamma=gamma_i\n",
    "        \n",
    "    \n",
    "    return min_gamma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aff9dd6",
   "metadata": {},
   "source": [
    "### Main Primal-Dual Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "01c5bd8f-769f-44dd-b990-e9e644835f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def primal_dual_opt(X,y,m,kernel_type,order,gap=10e-4,inner_tol=10e-1,weight_threshold=0.01,maxouter_iter=100,maxinner_iter=10 ,verbose=True):\n",
    "    \"\"\" Computes the optimal weights for the MKL objective using primal dual method\n",
    "    \"\"\"\n",
    "    \n",
    "    duality_gap=1 # initialize duality gap\n",
    "    C=0.01 # penalty constant for SVM\n",
    "    y_outer=np.outer(y,y) # outer product of y for efficiency\n",
    "    counter=0\n",
    "    d_m=np.ones(m)/m # initialize weights\n",
    "    D=np.ones(m) # initialize descent direction\n",
    "    mu=0 # initialize index of weight to be updated\n",
    "    line_search_steps=25 # number of steps for line search\n",
    "    gamma_max=0 # initialize step size\n",
    "    \n",
    "    # initialize kernel types\n",
    "    if kernel_type=='linear':\n",
    "        kernel_list=[Kernel(kernel_type=kernel_type)for i in range(1,order+1) ]\n",
    "    elif kernel_type=='polynomial':\n",
    "        \n",
    "        kernel_list=[Kernel(kernel_type,i) for i in range(1,order+1)]\n",
    "    elif kernel_type=='gaussian':\n",
    "        kernel_list=[Kernel(kernel_type,bandwidth=i) for i in np.linspace(0.1,1,m)] # gamma hyperparam \n",
    "\n",
    "    else:\n",
    "        print(\"Not Valid Kernel Type\")\n",
    "        return\n",
    "    \n",
    "    # stopping criteria is duality gap\n",
    "    while duality_gap>gap and gap>0:\n",
    "        if counter>maxouter_iter:\n",
    "            break\n",
    "        counter+=1\n",
    "\n",
    "        # compute svm objective\n",
    "        J_d,duality_gap,alpha=compute_dual(X,y,kernel_list,d_m,C) \n",
    "        if verbose:\n",
    "            print(\"Duality\",duality_gap)\n",
    "\n",
    "        # gradient wrt each kernel\n",
    "        gradient_j=[compute_gradient(i,X,y_outer,alpha) for i in kernel_list] \n",
    "        if verbose:\n",
    "            print(\"Gradient is \",gradient_j)\n",
    "   \n",
    "\n",
    "        # max element within d vector\n",
    "        mu=np.argmax(d_m)\n",
    "        grad_mu=gradient_j[mu]\n",
    "        \n",
    "        # computes normalized descent direction ; satisfies equality constraints \n",
    "        D=descent_direction(d_m,mu,gradient_j,grad_mu)\n",
    "        norm_D=np.sqrt(D.dot(D))\n",
    "        if norm_D==0.0:\n",
    "            break\n",
    "        D=D/norm_D\n",
    "        if verbose:\n",
    "            print(\"Descent Direction is \",D)\n",
    "        \n",
    "        # init descent direction update\n",
    "        J_hat=0\n",
    "        d_hat=d_m\n",
    "        D_hat=D\n",
    "        inner_iter=0\n",
    "        \n",
    "        ### Investigate zero gamma step size\n",
    "        gamma_list=[]\n",
    "\n",
    "        while J_hat+inner_tol<J_d or inner_iter<maxinner_iter:\n",
    "            inner_iter+=1 \n",
    "            \n",
    "            # indices where descent direction is negative, \n",
    "            # if none reached local max, else update by gamma step\n",
    "            nonzero_D=np.where(D_hat<0)[0]\n",
    "            if len(nonzero_D)==0:\n",
    "                gamma_max=0  \n",
    "            else:\n",
    "                gamma_max=np.min(-d_hat[nonzero_D]/D_hat[nonzero_D])\n",
    "\n",
    "            gamma_list.append(gamma_max)\n",
    "            D=D_hat\n",
    "            d_m=d_hat\n",
    "            d_hat=d_m+gamma_max*D\n",
    "            d_hat[d_hat<weight_threshold]=0\n",
    "                \n",
    "            D_hat[mu]=descent_direction(d_hat,mu,gradient_j,grad_mu)[mu]\n",
    "            J_hat=compute_dual(X,y,kernel_list,d_hat,C,compute_gap=False)\n",
    "            \n",
    "        # line search in descent direction\n",
    "        gamma_max=np.max(gamma_list)  \n",
    "        gamma_step=line_search(X,y,kernel_list,D,d_m,gamma_max,disc=line_search_steps)\n",
    "        \n",
    "       \n",
    "        d_m=(d_m+gamma_step*D)\n",
    "        if verbose:\n",
    "            print(\"Gamma Max is \",gamma_max)\n",
    "            print(\"Gamma Step is \",gamma_step)\n",
    "        \n",
    "        # normalize and drop threshold\n",
    "        d_m[d_m<weight_threshold]=0\n",
    "        d_m=d_m/np.sum(d_m)\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"Weights are \",d_m)\n",
    "       \n",
    "    if abs(duality_gap)<gap:\n",
    "            print(\"Duality Gap Reached\")\n",
    "            return d_m,kernel_list\n",
    "    else:\n",
    "        print(\"Max Iterations Reached\")\n",
    "        return d_m,kernel_list\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50e13da",
   "metadata": {},
   "source": [
    "### Form optimal Kernel as a linear combination of multiple kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dcc595a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def form_optimal_svm(kernel_list,d_m,X,y):\n",
    "    \"\"\" Forms optimal SVM from optimal weights and kernels\n",
    "    \"\"\"\n",
    "    # compute optimal kernel\n",
    "    K=np.zeros((X.shape[0],X.shape[0]))\n",
    "    for i in range(len(kernel_list)):\n",
    "        K+=d_m[i]*kernel_list[i].compute_kernel(X)\n",
    "    \n",
    "    # fit svm\n",
    "    clf=svm.SVC(kernel='precomputed')\n",
    "    clf.fit(K,y)\n",
    "    return clf,K"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577ed14f",
   "metadata": {},
   "source": [
    "### Test MKL Function on simulated Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "18d90af1-4d44-4e24-9f53-1d3aa4fcade4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_mkl(kernel_type,verbose=True):\n",
    "    \n",
    "    n=200# data set} size\n",
    "    \n",
    "    m=3 #6 num kernels\n",
    "    \n",
    "    y=np.sign(np.random.uniform(-1,1,size=n)) # sample class labels 1,-1\n",
    "    x=np.random.rand(n,5) # # sample features\n",
    "    \n",
    "    \n",
    "  \n",
    "    d_m,kernel_list=primal_dual_opt(x,y,m,kernel_type,order=m,verbose=verbose)\n",
    "    \n",
    "    print(\"Optimal Weights are \",np.round(d_m,2))\n",
    "\n",
    "    optimal_svm,K_composed=form_optimal_svm(kernel_list,d_m,x,y)\n",
    "    \n",
    "    # compute accuracy\n",
    "    print(\"In Sample Accuracy is \",optimal_svm.score(K_composed,y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ada27a",
   "metadata": {},
   "source": [
    "### Basis of polynomial kernels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0f336472",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duality 0.0229407254235555\n",
      "Gradient is  [-0.005203212310168938, -0.01676931298786309, -0.04539735078434912]\n",
      "Descent Direction is  [-0.77780302  0.17380422  0.6039988 ]\n",
      "Gamma Max is  0.428557518860869\n",
      "Gamma Step is  0.4114152181064342\n",
      "Weights are  [0.         0.36312435 0.63687565]\n",
      "Duality 0.010296674214020474\n",
      "Gradient is  [-0.005364062920025884, -0.016891446306318194, -0.04524722646645585]\n",
      "Descent Direction is  [ 0.         -0.70710678  0.70710678]\n",
      "Gamma Max is  0.5135353793324873\n",
      "Gamma Step is  0.02054141517329949\n",
      "Weights are  [0. 0. 1.]\n",
      "Duality 0.0\n",
      "Gradient is  [-6.434205543833542, -9.810268407130154, -15.85681675080259]\n",
      "Duality Gap Reached\n",
      "Optimal Weights are  [0. 0. 1.]\n",
      "In Sample Accuracy is  0.66\n"
     ]
    }
   ],
   "source": [
    "test_mkl(\"polynomial\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4731ef33",
   "metadata": {},
   "source": [
    "### Basis of Gaussian Kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "06c31006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duality 0.0002877149059912819\n",
      "Gradient is  [-5.20386656093114e-05, -0.00031469759039163625, -0.000614940486987353]\n",
      "Descent Direction is  [-0.7990716   0.25423119  0.54484042]\n",
      "Gamma Max is  0.41715076905550236\n",
      "Gamma Step is  0.4004647382932822\n",
      "Weights are  [0.        0.4099975 0.5900025]\n",
      "Duality Gap Reached\n",
      "Optimal Weights are  [0.   0.41 0.59]\n",
      "In Sample Accuracy is  0.575\n"
     ]
    }
   ],
   "source": [
    "test_mkl(\"gaussian\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b632aa83",
   "metadata": {},
   "source": [
    "### Main Results Learned\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa574042",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "50067003",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('research')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "8fefc22b81b58a95d090389e6427ac7414ba2434182c3be0d4e0279b8ec28ded"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
